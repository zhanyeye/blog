---
title: 19-10-17-数据采集方法(数据分析与挖掘)
mathjax: true
date: 2019-10-18 16:52:09
tags:
- python
categories:
---

##### 常见数据文件存储和读取

- 数据文件类型
- 数据文件读取
- 数据文件存储
- JSON 解析
- 数据分块读取

<!--more-->

###### 常用数据文件格式

 当我们使用 Python 读取数据文件时，首先推荐的就是通过 Pandas 完成，Pandas 几乎支持所有常见的数据文件格式。 

<img src="https://raw.githubusercontent.com/zhanyeye/Figure-bed/win-pic/img/20191017215316.png" width="280" height="280" alt="图片名称" align=center>

###### Excel和CSV格式

 由于 Excel 表格有最大的行数储存限制（16,384 列 × 1,048,576 行），所以更多时候我们会使用 CSV 来储存表格数据。 

>  CSV 的英文是 Comma-Separated Values，其实就是通过字符分割数据并以纯文本形式存储。这里的分割字符我们一般会使用逗号，所以往往也称 CSV 文件为逗号分隔符文件。纯文本意味着该文件是一个字符序列，不含必须像二进制数字那样被解读的数据，也没有最大行数的储存限制。 

 我们尝试读取 Excel 和 CSV 格式的示例数据文件。首先，我们需要生成不同类型的数据示例文件。下面这段代码直接点击运行即可，将会在目录下方生成两个最常用的数据文件 `test.csv` 和 `test.xlsx`。 

```python
import numpy as np
import pandas as pd

# 生成示例数据
df = pd.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10)})

# 写入数据文件
df.to_csv('test.csv', index=None)  # CSV
df.to_excel('test.xlsx', index=None)  # EXCEL
print("*****示例文件写入成功*****")
```

 当你读取 Excel 文件时，首先需要安装 `openpyxl` 模块，不然就会报错。安装该模块的命令为：`pip install openpyxl`。

 使用 Pandas 读取文件的方法，直接运用上面表格中的 API 即可。CSV 文件读取是 `read_csv` ，而 Excel 文件读取是 `read_excel` 

```
pd.read_csv("test.csv")
pd.read_excel('test.xlsx')
```

###### HDF5 格式

[ *HDF*](https://zh.wikipedia.org/zh-hans/HDF)（英语：Hierarchical Data Format）指一种为存储和处理大容量科学数据设计的文件格式及相应库文件。HDF5 格式的特点在于能存储多个数据集，并且支持 `metadata`。

HDF5 文件包含两种基本数据对象：

- 群组（group）：类似文件夹，可以包含多个数据集或下级群组。
- 数据集（dataset）：数据内容，可以是多维数组，也可以是更复杂的数据类型。

群组和数据集都支持元数据 `metadata`，用户可以自定义其属性，提供附加信息。元数据类似于「数据的数据」，它能够用来说明数据的特征和其他属性。

HDF5 的好处在于，你不仅可以使用 Python 存储和读取，目前还被 Java，MATLAB/Scilab，Octave，IDL，Julia, R 等语言或商业软件支持。

下面，我们同样尝试使用 Pandas 来存储和读取 HDF5 数据。和 Excel 文件读取相似，我们需要先安装一个依赖模块 PyTables，命令为：`pip install tables`。

```python
df1 = pd.DataFrame({'A': np.random.randn(10), 'B': np.random.randn(10)})  # 随机数据
df2 = pd.DataFrame({'C': np.random.randn(10), 'D': np.random.randn(10)})  # 随机数据

df1.to_hdf('test.h5', key='df1')  # 存储 df1
df2.to_hdf('test.h5', key='df2', format='table')  # 存储 df2 

"""
会发现上面我们在存储示例数据时，df2 后面指定了 format='table' 参数。这是因为，HDF 支持两种存储架构：fixed 和 table。默认为 fixed，因为其读取速度更快，但是 table 却支持查询操作。
"""
```

 我们通过指定 `key` 向 HDF 文件中存储了 2 个不同的数据集 `df1` 和 `df2`。那么，接下来我们尝试读取。 

```python
pd.read_hdf('test.h5', key='df1')  # 读取 df1 
pd.read_hdf('test.h5', key='df2', where=['index < 5']) # 读取 df2 中 index < 5 的数据
```

>  HDF5 既然支持存储多个数据集，是不是类似于数据库中的「表」呢？值得注意的是，HDF5 并不是数据库，如果多个使用者同时写入数据，数据文件会遭到破坏。 

###### JSON 格式

> JSON 数据格式与语言无关，脱胎于 JavaScript，但目前很多编程语言都支持 JSON 格式数据的生成和解析。JSON 的官方 MIME 类型是 `application/json`，文件扩展名是 `.json`。
>
> 这里特别说到 JSON 格式，原因是其已经成为了 HTTP 请求过程中的标准数据格式。而后面的采集数据过程中，我们会学习到通过 API 请求数据，一般都会对 JSON 进行解析。所以，这里先行了解学习。

 JSON 数据中 `key` 必须是字符串类型，缺失值用 `null` 表示。其中还可能包含的基本类型有：字典，列表，字符串，数值，布尔值等。 

 DataFrame 的确是最佳的数据呈现格式。不过，由于 JSON 支持复杂的嵌套，有时候直接通过 `read_json` 读取到的 DataFrame 并不是我们想要的样子，例如某个键值是以字典或列表存在。此时，我们就会用其他的工具来解析 JSON 了。 

 Python 中有许多能够储存和解析 JSON 的库，这里推荐使用内建库 `json`。

+  `json.loads(obj)` ：将json文件中的字符串转化为Python 的数据类型（Python Object）
+   `json.dumps` 可以把 Python Object 转换为 JSON 类型 

###### `read_` 操作参数详解

- `path`：路径不仅仅可以读取本地文件，还支持远程 URL 链接。
- `sep`：支持按特定字符分割。
- `header`：可以指定某一行为列名，默认是第一行。
- `names`：自定义列名。
- `skiprows`：指定忽略某些行。
- `na_values`：对空值进行指定替换操作。
- `parse_dates`：尝试将数据解析为日期。
- `nrows`：读取指定行数的数据。
- `chunksize`：指定分块读取数据大小。
- `encoding`：指定文件编码。

> 如果你的 CSV 文件是使用分号 `;` 而不是逗号 `,` 分割，那么就可以通过 `sep=';'` 让数据加载为正常的 DataFrame 格式。 

> 像 `skiprows` 非常常用，它可以指定忽略某些行。，使得在加载数据时就可以对数据实现过滤，面对庞大且加载较慢的数据文件时特别好用 

###### 分块读取数据

在很多时候，手中的数据集都非常大。例如当我们直接读取一个 GB 级别的 CSV 文件时，不仅速度很慢，还有可能因为内存不足而报错。此时，通过分块读取的方式加载数据文件就非常方便了。

 通过上面的 `read_` 参数可以看出，分块读取需要指定 `chunksize`，也就是每一块的大小 


```python
chunker = pd.read_csv("test.csv", chunksize=2)
chunker
'''
chunker 返回的 pandas.io.parsers.TextFileReader 是一个可迭代对象。你可以通过 get_chunk() 逐次返回每一个块状数据的内容。你可以尝试多次运行下方单元格，以查看每次迭代的结果。
'''
chunker.get_chunk() #每次只读2行数据
```

 分块读取是解决大文件读取慢的有效手段，但需要注意 `chunksize` 并不是 Pandas 中每个 `read_` 操作都支持的参数，这需要你在使用时通过官方文档确认。 

 另外，分块读取也不适宜用于解决读取部分数据的需求。例如，你需要读取某个数据的前 1 万条，应该直接使用切片，而非分块读取。当然，如果数据文件本身非常大，全部读取后切片会爆掉内存，此时才宜用分块读取进行解决。 



##### SQL和NoSQL数据库基础

- 数据库连接
- 操作 SQLite 数据库
- SQL 语法介绍
- MongoDB 数据库介绍
- 对 MongoDB 数据库的增删改查

> 除了数据文件，另外一种读取数据的途径就是直接连接数据库。Pandas 也支持直接连接 SQL 数据库以及 Google Big Query。SQL 数据库就是常见的关系型数据库，例如 MySQL，SQLite 等。而 BigQuery 是 Google 推出的可扩展性强、成本低廉的无服务器企业数据仓库，可让您的所有数据分析人员更加高效地工作。关于 BigQuery 的更多信息 [ *BigQuery 官网*](https://cloud.google.com/bigquery/) 

<img src="https://raw.githubusercontent.com/zhanyeye/Figure-bed/win-pic/img/20191019103426.png" width="210" height="75" alt="图片名称" align=center>

当我们连接数据库时，首先需要安装相应数据库的驱动程序库。例如 MySQL 需要安装 `pymysql`。由于 **SQLite 是 Python 的标准库**，所以下面我们通过 SQLite 来学习如何连接数据库。 

###### SQLite  数据库

 SQLite 是一个非常常用的关系型数据库。SQLite 具有很多优点，其中最突出的就是**无需服务器、也无需配置**。SQLite 是非常小且非常轻量，无需外部依赖，非常好用。 

 SQLite 是数据分析过程中非常推荐的数据库，当你需要备份或分享数据时，无需导入导出，直接将 SQLite 存储的 `.sqlite` 文件拷贝即可。 

1. 因为 SQLite 是 Python 标准库，只需要 `import sqlite3` 即可加载 SQLite [ *官方文档*](https://docs.python.org/zh-cn/3.7/library/sqlite3.html) 

2. `sqlite3.connect` 操作连接数据库，指定数据库名称之后，SQLite 会连接或创建相应的数据库文件 
3.  接下来，我们就可以向 `test.sqlite` 中写入数据了。直接使用 Pandas 中的 `to_sql` 即可 

```python
import os
import sqlite3
import pandas as pd
import numpy as np

# 如果目录下已存在 SQLite 数据库，执行删除避免重复运行报错
if os.path.exists('test.sqlite'):
    os.remove('test.sqlite')
    
sql_con = sqlite3.connect('test.sqlite')  # 连接数据库

# 生成示例数据
df = pd.DataFrame({'A': np.random.randn(50), 'B': np.random.randn(50)})

# 向数据库中写入示例数据，表名为 test_table
df.to_sql(name='test_table', con=sql_con, index=None)
sql_con.close()  # 关闭连接

'''
值得注意的是，如果你重复运行上面的单元格会报错，原因是名为 test_table 数据表已经存在了。那么，你可以更改表的名字再写入数据即可。
'''
```

 接下来，我们尝试通过 Pandas 来加载数据库中的数据。这个过程大致分为两个步骤，建立数据库连接，再使用 SQL 语句查询。 

```python
sql_con = sqlite3.connect('test.sqlite')  # 建立数据库连接
sql_query = "SELECT * FROM test_table"  # SQL 查询语句，查询 test_table 表中全部数据

pd.read_sql(sql_query, sql_con)  # 执行查询并输出数据
```



###### SQL SELECT 语法

详见：[Mysql基础课程]( [https://zhanyeye.netlify.com/19-07-21-mysql%E5%9F%BA%E7%A1%80%E8%AF%BE%E7%A8%8B/](https://zhanyeye.netlify.com/19-07-21-mysql基础课程/) )



###### MongoDB 数据库

<img width='300px' src="https://doc.shiyanlou.com/document-uid214893labid7506timestamp1539841077489.png">

 MongoDB 是非常流行的 NoSQL 数据库，支持自动化的水平扩展，同时也被称为文档数据库，因为数据按文档的形式进行存储（BSON 对象，类似于 JSON）。在 MongoDB 中数据存储的组织方式主要分为四级： 

- 数据库实例，比如一个 app 使用一个数据库；
- collection 文档集合 ，一个数据库包含多个文档集合，类似于 MySQL 中的表；
- document 文档，一个文档代表一项数据，类似于 JSON 对象，对应于 MySQL 表中的一条记录；
- 字段：一个文档包含多个字段；

 MongoDB 存储的数据可以是无模式的，比如在一个集合中的所有文档不需要有一致的结构。也就是说往同一个表中插入不同的数据时，这些数据之间不必有同样的字段。这和关系型数据库彻底不同，在关系型数据库中创建表时就已经确定了数据项的字段，向其中插入数据时，必须是相同的结构。 

 当我们使用 Python 操作 MongoDB 时，需要安装 [ *PyMongo*](https://github.com/mongodb/mongo-python-driver)。安装命令为 ：`pip install pymongo`

1. 使用 PyMongo 的第一步是创建一个 MongoClient 来运行 MongoDB 实例，这里我们连接本地主机与默认端口号 `27017`。 

   ```python
   from pymongo import MongoClient
   
   client = MongoClient('localhost', 27017)
   client
   --------
   MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)
   ```

2.  一个 MongoDB 的实例可以操作多个独立的数据库，我们使用 PyMongo 的时候可以通过 MongoClient 的属性来获取不同的数据库。这里，我们来获取 `shiyanlou` 数据库。 

   ```python
   db = client.shiyanlou
   db
   -------
   Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'shiyanlou')
   ```

3.  MongoDB 中的集合用来保存一组文档，相当于关系型数据库中的数据表。这里我们获取 `shiyanlou_collection`。 

   ```python
   db.shiyanlou_collection
   ```

   > 值得注意的是，上面的例子中，我们获取的 `shiyanlou` 数据库和 `shiyanlou_collection` 集合都是延迟创建的，也就是说执行上面的命令实际上不会在 MongoDB 的服务器端进行任何操作，只有当第一个文档插进去的时候，它们才会被创建。

4. MongoDB 存储的文档记录是一个 BSON 对象，类似于 JSON 对象，由键值对组成。比如一条用户记录：

   ```
   {
       name: "Aiden",
       age: 30,
       email: "luojin@simplecloud.cn"
   }
   ```

   每一个文档都有一个 `_id` 字段，该字段是主键，用于唯一的确定一条记录。如果往 MongoDB 中插入数据时没有指定 `_id` 字段，那么会自动产生一个 `_id` 字段，该字段的类型是 [ *ObjectId*](https://docs.mongodb.com/manual/reference/bson-types/#objectid)，长度是 12 个字节。在 MongoDB 文档的字段支持字符串，数字，时间戳等类型。一个文档最大可以达到 16M, 可以存储相当多的数据。

5.  接下来，使用 `insert_one()` 方法往 MongoDB 中插入一条数据： 

   ```python
   data = {'name': "Aiden", 'age': 30,
           'email': "luojin@simplecloud.cn", 'addr': ["CD", "SH"]}
   
   users = db.users # 创建一个users集合，用来保存user文档
   users_id = users.insert_one(data).inserted_id
   users_id
   ```

6.  插入第一个文档之后，集合 `users` 就被创建了，我们可以用 `list_collection_names` 查看数据库中已经创建好的集合： 

   ```python
   db.list_collection_names()
   ------
   ['users']
   ```

7.  接下来，我们向 `users` 集合中插入多条数据，可以使用 `insert_many` 方法： 

   ```python
   data = [{'name': 'lxttx', 'age': 28, 'email': 'zhanyeye@qq.com', 'addr': ['BJ', 'CD']},
           {'name': 'jin', 'age': 31, 'email': 'zhanhah@qq.com', 'addr':['GZ', 'SZ']},
           {'name': 'akk', 'age': 26, 'email': 'zhand@qq.com', 'addr': ['NJ', 'AH']}
          ]
   db.users.insert_many(data)
   -----
   <pymongo.results.InsertManyResult at 0x7f1cc2eb53c8>
   ```

8.  MongoDB 中最常用的基本操作是 `find_one()`，这个方法返回查询匹配到的第一个文档，如果没有则返回 `None`

   ```python
   users.find_one()  #参数为空，来获取 users 集合中的第一个文档
   -----
   {'_id': ObjectId('5daab08220e98f003263b302'),
    'name': 'Aiden',
    'age': 30,
    'email': 'zhanyeye@qq.com',
    'addr': ['CD', 'SH']}
   ```

   ```python
   users.find_one({'name': "jin"})
   --------
   {'_id': ObjectId('5daab37320e98f003263b306'),
    'name': 'jin',
    'age': 31,
    'email': 'zhanhah@qq.com',
    'addr': ['GZ', 'SZ']}
   ```

9.  为了查询多个文档，我们可以使用 `find()` 方法，`find()` 方法返回一个 Cursor 对象，使用这个对象可以遍历所有匹配的文档 

   ```python
   for user in users.find():
       print(user)
   ------
   {'_id': ObjectId('5daab08220e98f003263b302'), 'name': 'Aiden', 'age': 30, 'email': 'zhanyeye@qq.com', 'addr': ['CD', 'SH']}
   {'_id': ObjectId('5daab08b20e98f003263b303'), 'name': 'Aiden', 'age': 30, 'email': 'zhanyeye@qq.com', 'addr': ['CD', 'SH']}
   {'_id': ObjectId('5daab25b20e98f003263b304'), 'name': 'Aiden', 'age': 30, 'email': 'zhanyeye@qq.com', 'addr': ['CD', 'SH']}
   {'_id': ObjectId('5daab37320e98f003263b305'), 'name': 'lxttx', 'age': 28, 'email': 'zhanyeye@qq.com', 'addr': ['BJ', 'CD']}
   {'_id': ObjectId('5daab37320e98f003263b306'), 'name': 'jin', 'age': 31, 'email': 'zhanhah@qq.com', 'addr': ['GZ', 'SZ']}
   {'_id': ObjectId('5daab37320e98f003263b307'), 'name': 'akk', 'age': 26, 'email': 'zhand@qq.com', 'addr': ['NJ', 'AH']}
   ```

   ```python
   for user in users.find({'name': "jin"}):
       print(user)
   ----
   {'_id': ObjectId('5daab37320e98f003263b306'), 'name': 'jin', 'age': 31, 'email': 'zhanhah@qq.com', 'addr': ['GZ', 'SZ']}
   ```

10.  更新数据主要通过 `db.users.update_one` 或者 `db.users.update_many` 方法，前者更新一条记录，后者更新多条记录 

    ```python
    db.users.update_one(filter={'name': "Aiden"}, update={
                        '$set': {'age': 29, 'addr': ["CD", "SH", "BJ"]}})
    ```

11.  删除数据也非常简单，可以通过 `db.users.delete_one` 或`db.users.delete_many` 方法 

    ```python
    db.users.delete_many({'addr': "CD"})
    ```

12.  `estimated_document_count()` 方法可以使用集合元数据估算此集合中的文档数